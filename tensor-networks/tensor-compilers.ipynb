{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this about!?\n",
    "\n",
    "What are tensor-compilers for?\n",
    "Noticing structure in operations and mapping that onto hardware efficiently?\n",
    "\n",
    "https://research.fb.com/announcing-tensor-comprehensions/\n",
    "http://pytorch.org/2018/03/05/tensor-comprehensions.html\n",
    "\n",
    "\n",
    "\n",
    "[Tensor Comprehension](https://arxiv.org/abs/1802.04730)\n",
    "\n",
    "* but half the time a single op would be more efficient if it what had come before and what was coming after. so efficient ops is nice, but ...? (is this already handled by traditional compilers?)\n",
    "* polyhedral intermediate representation\n",
    "* abstraction without regret\n",
    "* how much smarter can we make the autotuner? when should it be used?\n",
    "\n",
    "***\n",
    "\n",
    "still doesnt get me what i want!? WANT;\n",
    "\n",
    "* if I have some tensor network representation, I dont want to construct the entire tensor, instead I want to do the ops on the cores/arms.\n",
    "* keep an online estimate of the total cost of operations, spend compute on finding new faster kernels. \n",
    "* ability to take in more context. \n",
    "* given some arbitrary tensor operation/representation, find efficient kernels.\n",
    "* ?\n",
    "\n",
    "***\n",
    "\n",
    "https://www.reservoir.com/wp-content/uploads/2017/08/RStreamTF.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (30) : unknown error at /opt/conda/conda-bld/pytorch_1518243271935/work/torch/lib/THC/THCGeneral.c:70",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-24630f186e9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtensordot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tensordot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mI0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mbest_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensordot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautotune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/datasci/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/datasci/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m     \u001b[0;31m# We need this method only for lazy init, so we can remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_CudaBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/datasci/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    141\u001b[0m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_sparse_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (30) : unknown error at /opt/conda/conda-bld/pytorch_1518243271935/work/torch/lib/THC/THCGeneral.c:70"
     ]
    }
   ],
   "source": [
    "import tensor_comprehensions as tc\n",
    "import torch\n",
    "lang = \"\"\"\n",
    "def tensordot(float(N, C1, C2, H, W) I0, float(N, C2, C3, H, W) I1) -> (O) {\n",
    "    O(n, c1, c3, h, w) +=! I0(n, c1, c2, h, w) * I1(n, c2, c3, h, w)\n",
    "}\n",
    "\"\"\"\n",
    "N, C1, C2, C3, H, W = 32, 512, 8, 2, 28, 28\n",
    "tensordot = tc.define(lang, name=\"tensordot\")\n",
    "I0, I1 = torch.randn(N, C1, C2, H, W).cuda(), torch.randn(N, C2, C3, H, W).cuda()\n",
    "best_options = tensordot.autotune(I0, I1, cache=True)\n",
    "out = tensordot(I0, I1, options=best_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The auto tune function is interesting. \n",
    "Keep track of how often a fn is called. Maybe autotune it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some fn, $f_{\\theta}(x)$, but we get to choose how to represent the parameters ($\\theta$), that is to say that we parameterise the parameters? $\\theta_{\\phi}$. After selecting a different representation, we can define a new op and autotune it.\n",
    "\n",
    "Want the representation is a function of the loss q.r.t those parameters and their speed/efficiency. \n",
    "\n",
    "* l = loss of the parameters. (how do you  caluclate this?)\n",
    "* t = time for the\n",
    "* \n",
    "\n",
    "$$\n",
    "r = g(l, t, m)\n",
    "$$\n",
    "\n",
    "\n",
    "Choosing a representation is independent of finding a fsat way to compute it. No? Cos we want to pick representations that are both fast and accurate?!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WANT to be able to simultaneously optimise;\n",
    "# parameters,\n",
    "# their representation,\n",
    "# and their compilation.\n",
    "\n",
    "# for starters let's consider each in isolation.\n",
    "# want los churn? changing the representation shouldnt effect loss much?\n",
    "\n",
    "\n",
    "# in the inference case. \n",
    "# we have frozen theta\n",
    "# and lets also freeze \n",
    "def fn():\n",
    "    for step in iters:\n",
    "\n",
    "        # make a prediction\n",
    "        y = model.predict(x)  # problem is that this only gives us one data point\n",
    "\n",
    "        param_loss = model.loss(wrt=theta)\n",
    "        rep_loss = g(param_loss, model.ops.theta.time, model.ops.theta.memory)\n",
    "        comp_loss = f\n",
    "\n",
    "        \n",
    "        model.update_representation(loss, )\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "# slightly different framing.\n",
    "loss = param + rep + comp  # <-- need a loss fn that can combine all\n",
    "\n",
    "# must choose which one to spend compute on for the next iteration.\n",
    "# maybe optimising params on data, \n",
    "# maybe decomposing params according to their structure,\n",
    "# maybe improving the numerical algols\n",
    "\n",
    "# so want to know current rate of improvement of \n",
    "# dL/dparam, dL/drep, dL/dcomp per compute spent.\n",
    "# is d(dL/param)/dcompute is getting quite small \n",
    "# then maybe we should focus somewhere else.\n",
    "\n",
    "\n",
    "# does this reduce back to hyperparameter optimisation!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need a representation for $\\theta_{\\phi}$ that can be optimised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try a simple case, \n",
    "# learn a linear projection. \n",
    "# however the projection has \n",
    "# some sort of structure in it (some sort of TN?)\n",
    "# which we would also like to learn. <-- seems hard? how!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss(x, t, model):\n",
    "    # theta in [n x m]\n",
    "    # how can we back prop to phi?\n",
    "    theta_approx = representation(theta, phi)\n",
    "    \n",
    "    # maybe need two copies of theta?\n",
    "    # at what point can we throw away alternative representations fo theta?\n",
    "    y = fn(x, theta)\n",
    "    \n",
    "    # weight each loss according to how much we care.\n",
    "    param_loss = compare(y, t)\n",
    "    rep_loss = difference(theta - theta_approx)  # this measure doesnt make sense?\n",
    "    comp_loss = f(fn.ops.theta.time, model.ops.theta.memory)\n",
    "    \n",
    "    # want to pick phi, theta according to this loss\n",
    "    # also want to choose how to spend compute?\n",
    "    # finding representations vs fast kernels vs parameter settings\n",
    "    return param_loss + rep_loss + comp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation(theta, phi):\n",
    "    # given a set of \n",
    "    \n",
    "    return theta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
