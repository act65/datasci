{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "In [Novikov eta al. 2016](http://papers.nips.cc/paper/5787-tensorizing-neural-networks.pdf) they use the tensor-train representation to construct a weight matrix. However, the tensor-train constructs a high dimensional teensor and they simply reshape it into a matrix. I thought this was interesting/weird and want to investigate. \n",
    "\n",
    "Specifically, I am interested in how parameters are shared across the constructed weight matrix. Weight tying is an important part of designing neural networks, and I am interested in the relationship between parameter tying schemes and tensor (networks) and reshaping.\n",
    "\n",
    "The motivating example would be that a convolution can be written as a parameter sharing scheme in matrix form. Constructed using a circulant, ...?!\n",
    "\n",
    "***\n",
    "\n",
    "Secondly, when looking at the [algol for HOSVD](https://lirias.kuleuven.be/bitstream/123456789/72517/1/94-31.pdf) there is an unfolding (aka reshape) operation that is used to matricse the tensors so the left singular vectors of each dimension can be calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tensor train format (aka MPS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import sympy as sym\n",
    "sym.init_printing(use_latex='mathjax')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NEED a way to visualise!\n",
    "\n",
    "# dont want to just focus on TT format.\n",
    "# what other interesting ones are there? \n",
    "# Have a look at MPS and PEPS?\n",
    "# something more exotic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 'abcdefghijkl'  # names for indexes\n",
    "\n",
    "def construct_core(idx, n):\n",
    "    # construct a 3-tensor\n",
    "    return sym.tensor.Array([[[sym.Symbol('{}_{}{}{}'.format(idx,i,j,k)) \n",
    "                               for i in range(n)] \n",
    "                              for j in range(n)] \n",
    "                             for k in range(n)])\n",
    "    \n",
    "def construct_cores(N, n):\n",
    "    return [construct_core(idx[i], n) for i in range(N)]\n",
    "\n",
    "def construct_tensor(cores):\n",
    "    t = cores[0]\n",
    "    for i in range(len(cores)-1):\n",
    "        t = sym.tensorproduct(t, cores[i+1])\n",
    "        t = sym.tensorcontraction(t, (3,4))  # not sure if this is right...\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "n_cores = 2\n",
    "cores = construct_cores(n_cores, 2)\n",
    "t = construct_tensor(cores)\n",
    "print(t.shape)\n",
    "\n",
    "t = sym.simplify(t)\n",
    "t = sym.reshape(t, [8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}a_{000} \\left(b_{000} + b_{011}\\right) & a_{000} \\left(b_{100} + b_{111}\\right) & a_{100} \\left(b_{000} + b_{011}\\right) & a_{100} \\left(b_{100} + b_{111}\\right) & a_{010} \\left(b_{000} + b_{011}\\right) & a_{010} \\left(b_{100} + b_{111}\\right) & a_{110} \\left(b_{000} + b_{011}\\right) & a_{110} \\left(b_{100} + b_{111}\\right)\\\\a_{001} \\left(b_{000} + b_{011}\\right) & a_{001} \\left(b_{100} + b_{111}\\right) & a_{101} \\left(b_{000} + b_{011}\\right) & a_{101} \\left(b_{100} + b_{111}\\right) & a_{011} \\left(b_{000} + b_{011}\\right) & a_{011} \\left(b_{100} + b_{111}\\right) & a_{111} \\left(b_{000} + b_{011}\\right) & a_{111} \\left(b_{100} + b_{111}\\right)\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡a₀₀₀⋅(b₀₀₀ + b₀₁₁)  a₀₀₀⋅(b₁₀₀ + b₁₁₁)  a₁₀₀⋅(b₀₀₀ + b₀₁₁)  a₁₀₀⋅(b₁₀₀ + b₁₁₁\n",
       "⎢                                                                             \n",
       "⎣a₀₀₁⋅(b₀₀₀ + b₀₁₁)  a₀₀₁⋅(b₁₀₀ + b₁₁₁)  a₁₀₁⋅(b₀₀₀ + b₀₁₁)  a₁₀₁⋅(b₁₀₀ + b₁₁₁\n",
       "\n",
       ")  a₀₁₀⋅(b₀₀₀ + b₀₁₁)  a₀₁₀⋅(b₁₀₀ + b₁₁₁)  a₁₁₀⋅(b₀₀₀ + b₀₁₁)  a₁₁₀⋅(b₁₀₀ + b₁\n",
       "                                                                              \n",
       ")  a₀₁₁⋅(b₀₀₀ + b₀₁₁)  a₀₁₁⋅(b₁₀₀ + b₁₁₁)  a₁₁₁⋅(b₀₀₀ + b₀₁₁)  a₁₁₁⋅(b₁₀₀ + b₁\n",
       "\n",
       "₁₁)⎤\n",
       "   ⎥\n",
       "₁₁)⎦"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can even cosntruct tensors where we have the same \n",
    "# or more parameters than elements, but they are shared\n",
    "# in interesting ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "s = str(t) \n",
    "print(s.count('a_000'))\n",
    "print(s.count('a_001'))\n",
    "\n",
    "# so each parameter is shared over eight spaces?\n",
    "# want a general formula for this\n",
    "\n",
    "# also there is an a_{}{}{} parameter in every element.\n",
    "\n",
    "# kind of locality prior?\n",
    "# each parameter is shared over some local set (a row or colum)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The various forms of tensor SVD\n",
    "\n",
    "So, what about all the reshaping funny business going on in HSVD and HOSVD?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfold(tensor, axis): # aka Mode, matricization, fibre\n",
    "    return np.reshape(tensor, (tensor.shape[axis], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not have any good intuition for why/how taking the\n",
    "left eigenvectors of a reshaped tensor ...?\n",
    "so somehow, under reshaping, the left singular vectors are preserved?\n",
    "S and V are unncessary (which seems rather unusual...)\n",
    "\n",
    "***\n",
    "The way the core singular value tensor is calucluated seems like cheating.\n",
    "$$\n",
    "\\mathcal A = S \\times_1 U_1 ... \\times_n U_n \\\\\n",
    "S = \\mathcal A \\times_1 U_1^T ... \\times_n U_n^T \\\\\n",
    "$$\n",
    "this doesnt seem right, S should be diagonal!?\n",
    "\n",
    "***\n",
    "\n",
    "Hierarchical SVD also uses the same trick.\n",
    "Should I bother coding it?\n",
    "Seems interesting as now you have multiple core tensors and they need to be reconstructed using the right graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOSVD():\n",
    "    def decompose(tensor):\n",
    "        U = []\n",
    "        # for each arm of the tensor\n",
    "        for i, s in enumerate(tensor.shape):\n",
    "            u, _, _ = np.linalg.svd(unfold(tensor, i))\n",
    "            U.append(u)\n",
    "\n",
    "        S = tensor\n",
    "        for i, leg in enumerate(U):\n",
    "            S = np.tensordot(leg.T, S, axes=[1, i])\n",
    "        return U, S\n",
    "\n",
    "    def construct(legs, core):\n",
    "        c = core\n",
    "        # or could outerproduct the legs first and then elementwise mul!?\n",
    "        for i, leg in enumerate(legs):\n",
    "            c = np.tensordot(leg, c, axes=[1, i])\n",
    "        return c\n",
    "    \n",
    "    def test():\n",
    "        A = np.random.random((5,5,5))\n",
    "        u, s = HOSVD.decompose(A)\n",
    "        B = HOSVD.construct(u ,s)\n",
    "        d = np.sum(np.abs(A - B))\n",
    "        if d > 1e-8:\n",
    "            raise ValueError('A and B are not equal. Difference = {}'.format(d))\n",
    "            \n",
    "HOSVD.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so that is the motivation out of the way... phew. Now lets take a closer look at reshaping.\n",
    "\n",
    "Main questions;\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping algols\n",
    "\n",
    "* Row first, outer vs inner.\n",
    "* Can just be done at read time with different striding patterns (see views in numpy?)\n",
    "\n",
    "Only real requirements are that is must have an inverse? It is consistent? It ...?\n",
    "What about more 'random' permutations on the indexes?\n",
    "\n",
    "What if we thought about it as a function? What does a reshape do?\n",
    "Have some $f: x->y$ but we change the f while preserving XXX?!? What is preserved? What are its symmetries?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(tensor):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is reshaing a linear op!?\n",
    "Does it commute, associate, distribute, ...\n",
    "Firstly, its a unary operation?! So not sure what to do with that...\n",
    "\n",
    "### Associativity\n",
    "\n",
    "$\\varrho(u) + (v + w) = (\\varrho(u) + v) + w$\n",
    "\n",
    "### Commutativity\n",
    "\n",
    "$\\varrho(a) + b = b + \\varrho(a)$\n",
    "\n",
    "\n",
    "$a(\\mathring u + v) = \\mathring{au} + av$\n",
    "\n",
    "\n",
    "\n",
    "Reshaping is a permutation of the bases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Reshape.\n",
    "Want;\n",
    "- some properties that I can measure!?!\n",
    "- some visualisations! (what happens when I reshape?)\n",
    "- better intuition... need a concrete example to play with\n",
    "-\n",
    "\n",
    "#### Neighborhoods\n",
    "\n",
    "Picture I already have. Neighbors and where they go to.\n",
    "\n",
    "\n",
    "#### Connectedness (the dual of neighborhoods?)\n",
    "\n",
    "What about the graph POV?\n",
    "\n",
    "#### How is reshape like a convolution?\n",
    "\n",
    "For example, this is what we do when we want to do a convolution. Construct a tensor of patches (examples, X, Y, kernel, kernel) and then reshape it into a (examples x X x Y, kernel x kernel ) matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter sharing\n",
    "\n",
    "What is it, why do we do it, some examples.\n",
    "Can represent large(r) spaces with fewer parameters (that is the usual argument for TNs on parameter sharing...)\n",
    "\n",
    "Sharing over;\n",
    "\n",
    "* space,\n",
    "* time,\n",
    "* relations,\n",
    "* ?\n",
    "\n",
    "Nice way to build priors about invariance. (how does this related to the structure of tensor networks!?)\n",
    "\n",
    "Aka, parameter sharing schemes. If we write the reshaped, constructed tensor, and show the receptive field of original parameters.\n",
    "- are the receptive fields local, which tensor-nets/reshapings give local receptive fields?\n",
    "- ?\n",
    "-\n",
    "\n",
    "This idea is orthogonal to reshaping, reshaping is just a nice way to visualise it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\\begin{aligned}\n",
    "&= \\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} & a_{14} & a_{15} & a_{16} \\\\\n",
    "a_{21} & a_{22} & a_{23} & a_{24} & a_{25} & a_{26} \\\\\n",
    "a_{31} & a_{32} & a_{33} & a_{34} & a_{35} & a_{36} \\\\\n",
    "a_{41} & a_{42} & a_{43} & a_{44} & a_{45} & a_{46} \\\\\n",
    "a_{51} & a_{52} & a_{53} & a_{54} & a_{55} & a_{56} \\\\\n",
    "a_{61} & a_{62} & a_{63} & a_{64} & a_{65} & a_{66} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "&\\text{(stack by columns. reshape by first indices fastest)}\\\\\n",
    "&= \\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "a_{11} &  a_{31} & a_{51}\\\\\n",
    "a_{21} & a_{41} & a_{61}\\\\\n",
    "\\end{bmatrix} & \n",
    "\\begin{bmatrix}\n",
    "a_{12} &  a_{32} & a_{52}\\\\\n",
    "a_{22} & a_{42} & a_{62}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\begin{bmatrix}\n",
    "a_{13} &  a_{33} & a_{53}\\\\\n",
    "a_{23} & a_{43} & a_{63}\\\\\n",
    "\\end{bmatrix} & \n",
    "\\begin{bmatrix}\n",
    "a_{14} &  a_{34} & a_{54}\\\\\n",
    "a_{24} & a_{44} & a_{64}\\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\begin{bmatrix}\n",
    "a_{15} &  a_{35} & a_{55}\\\\\n",
    "a_{25} & a_{45} & a_{65}\\\\\n",
    "\\end{bmatrix} & \n",
    "\\begin{bmatrix}\n",
    "a_{16} &  a_{36} & a_{56}\\\\\n",
    "a_{26} & a_{46} & a_{66}\\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\end{bmatrix}\\end{aligned}$$\n",
    "\n",
    "Distances are not preserved. Originally $a_{33}$ is one index away from\n",
    "$a_{32},a_{34},a_{23},a_{43}$. But after the reshaping, the set of\n",
    "elements that d=1 are $a_{13},a_{53},a_{43},a_{31},a_{35},a_{34}$.\n",
    "If we map these back into the original matrix, we can see that the\n",
    "‘range’ of the indicies is speading. More are in each elements\n",
    "neighbourhood. What does this mean?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
