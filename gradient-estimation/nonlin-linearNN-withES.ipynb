{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this interesting?\n",
    "\n",
    "Depending on how we caluclate gradients, some things are learnable while others are not. \n",
    "Linear networks (trained with SGD) have been known not to ...\n",
    "Given a better teacher, it is possible to ... \n",
    "\n",
    "\n",
    "\n",
    "The difference between model-based and model-free optimisation. \n",
    "The model-based optimisation (SGD -- where we use a linear model, ie the gradient) is very close to being accurate, but is not quite true. VS. Model-free optimisation doesnt care, it just looks at outputs and ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "import torch as tr\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = tr.uint8\n",
    "n_pop = 128\n",
    "hidden_size = 64\n",
    "batch_size = 512\n",
    "n_hidden_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 8, 8) float64\n"
     ]
    }
   ],
   "source": [
    "ds = sklearn.datasets.load_digits(n_class=10, return_X_y=False)\n",
    "print(ds.images.shape, ds.images.dtype)\n",
    "images = tf.reshape(tf.cast(ds.images, dtype), (-1, 64))\n",
    "labels = tf.cast(ds.target, tf.int64)  # tf.one_hot(ds.target, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, l, params, get_acc=False):\n",
    "    # TODO. tensorflow doesnt have a lowdim matmul available in the python api...\n",
    "    # https://github.com/google/gemmlowp\n",
    "    h = tf.matmul(x, params['weights_first']) + params['biases_first']\n",
    "    for i in range(n_hidden_layers):\n",
    "        h = tf.matmul(h, params['weights_{}'.format(i)]) + params['biases_{}'.format(i)]\n",
    "    y = tf.matmul(h, params['weights_last']) + params['biases_last']\n",
    "\n",
    "    p = tf.nn.softmax(y)\n",
    "    if get_acc:\n",
    "        return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(p, axis=1), l), \n",
    "                             tf.float32))\n",
    "    else:\n",
    "        return tf.losses.sparse_softmax_cross_entropy(labels=l,logits=y)\n",
    "    \n",
    "params = {'weights_first': tf.random_normal((64, hidden_size), stddev=np.sqrt(2./64), dtype=dtype), \n",
    "          'biases_first': tf.random_normal((1, hidden_size), stddev=np.sqrt(2./64), dtype=dtype),\n",
    "          'weights_last': tf.random_normal((hidden_size, 10), stddev=np.sqrt(2./hidden_size), dtype=dtype), \n",
    "          'biases_last': tf.random_normal((1, 10), stddev=np.sqrt(2./hidden_size), dtype=dtype)}\n",
    "weights = {'weights_{}'.format(i): tf.random_normal((hidden_size, hidden_size), \n",
    "                                                    stddev=np.sqrt(2./hidden_size), \n",
    "                                                    dtype=dtype)\n",
    "          for i in range(n_hidden_layers)}\n",
    "biases = {'biases_{}'.format(i): tf.random_normal((1, hidden_size), \n",
    "                                                    stddev=np.sqrt(2./hidden_size), \n",
    "                                                    dtype=dtype)\n",
    "         for i in range(n_hidden_layers)}\n",
    "params = dict(params, **weights)\n",
    "params = dict(params, **biases)\n",
    "\n",
    "# def create(shape, dtype):\n",
    "#     return tf.cast(tf.random_uniform(shape, minval=-2**7, maxval=2**7), dtype)\n",
    "\n",
    "# params = {'weights1':create((64, hidden_size), dtype), \n",
    "#           'biases1': create((1, hidden_size), dtype),\n",
    "#           'weights2': create((hidden_size, 10), dtype), \n",
    "#           'biases2': create((1, 10), dtype)}\n",
    "\n",
    "o = func(images, labels, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(M, v):\n",
    "    shape = M.get_shape()\n",
    "    M = tf.reshape(M, (tf.shape(M)[0], -1))\n",
    "    sim = tf.losses.cosine_distance(tf.nn.l2_normalize(x, 0), tf.nn.l2_normalize(y, 0), dim=0)\n",
    "    M *= sim\n",
    "    return tf.reshape(M, shape)\n",
    "\n",
    "def create_noise(params, N, momentum, stddev):\n",
    "    return {k: tf.random_normal((N, v.shape[0], v.shape[1]), \n",
    "                                dtype=dtype, \n",
    "                                stddev=stddev)\n",
    "           for k, v in params.items()}\n",
    "\n",
    "def mutate_and_evaluate(func, params, images, labels, n_pop, n_iters, step, momentum):\n",
    "    noise = create_noise(params, n_pop, momentum, stddev=0.1*(1-step/n_iters))\n",
    "    losses = []\n",
    "\n",
    "    # for each elem in the population\n",
    "    for i in range(n_pop):  # this should be parallelised!\n",
    "        current_params = {k: params[k]+v[i, ...]\n",
    "                         for k, v in noise.items()}\n",
    "\n",
    "        # do we really need to evaluate on all the data?\n",
    "        idx = np.random.randint(0, images.shape[0]-batch_size)\n",
    "        # if I am going to do this I really need some mometum\n",
    "        # as this algol has no memory. so good results are forgotten\n",
    "\n",
    "        # use our fittness fn/loss fn to evaluate\n",
    "        loss = func(images[idx: idx+batch_size, ...], \n",
    "                    labels[idx: idx+batch_size, ...], \n",
    "                    current_params)\n",
    "        losses.append(loss)\n",
    "    #     print({k: v.shape for k,v in current_params.items()})\n",
    "\n",
    "    losses = tf.reshape(tf.stack(losses), (1, n_pop))\n",
    "    return noise, losses\n",
    "\n",
    "def update_params(losses, noise, params, \n",
    "                  lr=1.0, weighted_sum=True):\n",
    "    new_params = {}\n",
    "    m, v = tf.nn.moments(losses, 1)\n",
    "    losses = (losses - m)/v\n",
    "\n",
    "    momentum = {}\n",
    "    for k in params.keys():\n",
    "        # take a weighted sum\n",
    "        if weighted_sum:\n",
    "            dp = tf.reshape(tf.matmul(losses, \n",
    "                                      tf.reshape(noise[k], \n",
    "                                                (n_pop, -1))),\n",
    "                            params[k].shape)\n",
    "        # or could just pick the best. argmax.\n",
    "        else:\n",
    "            # this one seems to work better!?\n",
    "            dp = -noise[k][tf.argmin(losses, axis=1)[0]]\n",
    "            # elif:\n",
    "            # would also like to be able to sub in gradients from AD\n",
    "        \n",
    "        momentum[k] = dp\n",
    "\n",
    "#         print(np.sum(np.abs(dp)))\n",
    "        # do gradient descent on the params\n",
    "        new_params[k] = params[k] - lr*dp\n",
    "\n",
    "    return new_params, momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimiser():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class ES(Optimiser):\n",
    "    pass\n",
    "\n",
    "class SGD(Optimiser):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 99 loss: 0.8074568510055542"
     ]
    }
   ],
   "source": [
    "iters=1000\n",
    "momentum = {k: tf.zeros_like(v) for k,v in params.items()}\n",
    "for i in range(iters):\n",
    "    noise, losses = mutate_and_evaluate(func, params, images, labels, n_pop, iters, i, momentum)\n",
    "    params, momentum = update_params(losses, noise, params)\n",
    "#     print(np.mean(losses))\n",
    "    acc = func(images, labels, params, get_acc=True)\n",
    "    print('\\rstep: {} loss: {}'.format(i, acc), end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(method, N):\n",
    "    if method is 'es':\n",
    "        pass\n",
    "    if method is 'sgd':\n",
    "        pass\n",
    "    \n",
    "    \n",
    "es_loss = train('es', 3)\n",
    "sgd_loss = train('sgd', 3)\n",
    "\n",
    "plot(es_loss)\n",
    "plot(sgd_loss)\n",
    "# want to see that; \n",
    "# - es is slow\n",
    "# - sgd achieves lower accuracy (on linear nets)\n",
    "# - momentum!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.openai.com/nonlinear-computation-in-linear-networks/\n",
    "\n",
    "> As a consequence of these conventions and the binary format used, the smallest normal non-zero number (in binary) is 1.0..0 x 2^-126, which we refer to as min going forward. However, the next representable number is 1.0..01 x 2^-126, which we can write as min + 0.0..01 x 2^-126. It is evident that the gap between the 2nd number is by a factor of 2^20 smaller than gap between 0 and min. In float32, when numbers are smaller than the smallest representable number they get mapped to zero. Due to this ‘underflow’, around zero all computation involving floating point numbers becomes nonlinear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok cool. But how to train it...\n",
    "\n",
    "https://arxiv.org/abs/1703.03864\n",
    "\n",
    "https://gist.github.com/karpathy/77fbb6a8dac5395f1b73e7a89300318d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions to investigate;\n",
    "\n",
    "* When does it do better than SGD?\n",
    "* What if we instead used ES to estimate gradients and bproped them?\n",
    "* How can we get higher performance? \n",
    "    * More non-linearity? \n",
    "    * Momentum for ES?\n",
    "    * ?\n",
    "* What about overflow? What about `tf.float16`s or `tf.bfloat16` (16-bit truncated floating-point) or `tf.qint8` (Quantized 8-bit signed integer) or `tf.uint8`?\n",
    "* How do estimates of gradients effect generalisation?\n",
    "\n",
    "While it may be a 'linear' network, it has lost associativity, and cannot be factored back down to a single linear op. Lost interpretability. Effectively just a deep network, with step functions as the activations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
