{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key is that we have a connected space that undergoes a continuous transformation. (is this true?!?)\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d f(x)}{dx} &= \\mathop{lim}_{\\delta \\rightarrow 0} \\frac{f(x+\\delta) - f(x)}{\\delta} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "$\\forall x,\\epsilon \\exists \\delta: \\epsilon > XX - (\\frac{f(x + \\delta) - f(x)}{\\delta})$. We can make the approximation error ($\\epsilon$) arbitrarily small by reducing $\\delta$ (the local/linear approximation `approaches' the true gradient). This seems weird as in practice we use learning rates ($\\alpha$) that are much greater that $\\delta$. So we would expect that the local estimates of the gradient are not accurate unless ???.\n",
    "\n",
    "This seems interesting?!? Related to linear algebra? But we really care about locally linear, not necessarily linear algebra itself?\n",
    "\n",
    "\\subsection{Incremental changes}\n",
    "\n",
    "How can we make incremental changes efficient? And how does this relation to incremental lambda calculus and memoization?\n",
    "\n",
    "Want: ???\n",
    "\n",
    "Is this really just a more efficient version of forward AD?\n",
    "\n",
    "The reason we use reverse mode AD is because we have lost of inputs. But, by only considering changes, this greatly reduces the number of inputs. We could also parallelise the computation to give a feasible $\\mathcal O(t)$ algorithm for updating weights?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
