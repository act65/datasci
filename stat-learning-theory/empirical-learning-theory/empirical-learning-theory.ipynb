{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal, to be able to take the approximator fn class, data and learning algol and predict learnability.\n",
    "\n",
    "Learnability defined as? How `often`(from random inits) a `good`(low loss on test set?) `solution`(converged?) is learnt?\n",
    "\n",
    "- If $\\mathcal C_{data} > \\mathcal C_{model}$ then not learnable.\n",
    "- If $\\mathcal C_{data} < \\mathcal C_{model}$ then learnable (but might overfit?).\n",
    "- If $\\mathcal C_{data} << \\mathcal C_{model}$ then not learnable (hmm, empirically not true...).\n",
    "\n",
    "\n",
    "Note; we only get to control the model, thus the focus on representional capacity measures?\n",
    "\n",
    "OK. So the goal is. We measure the `complexity` of the data in various ways, and then correlate them with `learnability` (in this case, various models and optimisers?).\n",
    "\n",
    "## Representational complexity (/Capacity measures)\n",
    "\n",
    "Capacity measures allow us to cheat? As we use some form of Occam's razor assumption. \n",
    "The 'simplest' function that can fit the data will generalise the best.\n",
    "BUT, this is complicated by the fact that 'fit' may not be perfect. Thus we may need to trade-off fit and simplicity.\n",
    "\n",
    "Will try some alternatives? \n",
    "- A couple of NN architectures (will need to be scale according to size of data???)\n",
    "- Boosting algols? Xgboost? Maybe using decision trees?\n",
    "- SVM?\n",
    "\n",
    "## Data complexity\n",
    "\n",
    "How can this be measured?\n",
    "- Data dimension/manifold dim?\n",
    "- Topology of decision boundaries (for classifiers)\n",
    "- Moments of the data?\n",
    "- Smoothness?\n",
    "- how close decision boundaries are to each other.\n",
    "- Persistent homology. Barcode diagrams. Topology of NN networks formed by expanding balls around the data points.\n",
    "- if there is a train and test set then we would like to measure the difference.\n",
    "- \n",
    "\n",
    "\n",
    "\n",
    "On Characterizing the Capacity of Neural Networks using Algebraic Topology\n",
    "\n",
    "Is how the data arrives (ie online, delayed, ...) a problem within data complexity or within the optimisation algol?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### VC dimension\n",
    "\n",
    "What are they up to with the VC dimension? \n",
    "Goal is to be able to match representational capacity with complexity in the data. So, the VC dimension allows us to ask whether the function class we are learning from is able to shattern the data we have been given.\n",
    "\n",
    "If we can measure both then we might give accurate predictions of how learning will proceed.\n",
    "\n",
    "But, it is quite broken. Just being able to shattern something doesnt mean that it is possible to learn that shatterning. It doesnt take into account any learning dynamics.\n",
    "\n",
    "So a lower VC dimension means less possibly functions are representable. Thus(?) making it harder to overfit.\n",
    "\n",
    "A measure of flexibility (but also complexity).\n",
    "Can the fn shatter all assignments of labels. Is it flexible enough to move decision boundaries around as required.\n",
    "Relatedly, is it complex enough to contort the decision boundary as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# want a set of different data. measure their complexity\n",
    "# use UCI datasets and tensorflow plug-in?\n",
    "# https://www.kaggle.com/uciml/datasets\n",
    "# https://github.com/JackDunnNZ/uci-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sci\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/home/act65/.kaggle/datasets/uciml/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "def get_data_paths(basedir):\n",
    "    datafiles = []\n",
    "    for root, dirs, files in os.walk(basedir):\n",
    "    #     print(root, dirs, files)\n",
    "        for f in files:\n",
    "            if f.endswith('.csv'):\n",
    "                if 'assay' not in root:\n",
    "                    datafiles.append(os.path.join(root, f))\n",
    "    return datafiles\n",
    "\n",
    "fnames = get_data_paths(basedir)\n",
    "print(len(fnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['/home/act65/.kaggle/datasets/uciml/youtube-comedy-slam/comedy_comparisons-train.csv',\n",
       "  '/home/act65/.kaggle/datasets/uciml/aps-failure-at-scania-trucks-data-set/aps_failure_training_set_processed_8bit.csv',\n",
       "  '/home/act65/.kaggle/datasets/uciml/aps-failure-at-scania-trucks-data-set/aps_failure_training_set.csv'],\n",
       " ['/home/act65/.kaggle/datasets/uciml/youtube-comedy-slam/comedy_comparisons-test.csv',\n",
       "  '/home/act65/.kaggle/datasets/uciml/aps-failure-at-scania-trucks-data-set/aps_failure_test_set_processed_8bit.csv',\n",
       "  '/home/act65/.kaggle/datasets/uciml/aps-failure-at-scania-trucks-data-set/aps_failure_test_set.csv'])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = [f for f in fnames if 'train' in f]\n",
    "test = [f for f in fnames if 'test' in f]\n",
    "print(len(train), len(test))\n",
    "train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [pd.read_csv(f) for f in fnames]\n",
    "ds = datasets[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_categorical(data):\n",
    "    mapping = dict(zip(set(data), range(len(data))))\n",
    "    # returns {'category1': 0, 'category2': 1, ...}\n",
    "    return [mapping[d] for d in data]\n",
    "\n",
    "def cast_strings(ds):\n",
    "    for col in ds.columns.values:\n",
    "        if ds[col].dtype not in ['int64', 'float64']:\n",
    "            # probably want to keep track of which cols were categorical??\n",
    "            ds[col] = hash_categorical(ds[col])\n",
    "    return ds\n",
    "\n",
    "ds = cast_strings(ds)\n",
    "arr = np.array(ds)  # could use tf from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = [np.array(cast_strings(ds), dtype=np.float32) for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((916, 2), dtype('float32')), ((130, 21), dtype('float32')), ((2339, 3), dtype('float32')), ((177, 2), dtype('float32')), ((138, 19), dtype('float32')), ((1314, 2), dtype('float32')), ((1161, 5), dtype('float32')), ((702, 2), dtype('float32')), ((330, 2), dtype('float32')), ((225592, 3), dtype('float32')), ((912968, 3), dtype('float32')), ((536, 10), dtype('float32')), ((1599, 12), dtype('float32')), ((16000, 171), dtype('float32')), ((16000, 171), dtype('float32')), ((60000, 171), dtype('float32')), ((60000, 171), dtype('float32')), ((310, 7), dtype('float32')), ((310, 7), dtype('float32')), ((583, 11), dtype('float32'))]\n"
     ]
    }
   ],
   "source": [
    "print([(arr.shape, arr.dtype) for arr in arrays])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(f):\n",
    "    df = pd.read_csv(f)\n",
    "    df = cast_strings(df)\n",
    "    return np.array(df, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = [parse_dataset(f) for f in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = [parse_dataset(f) for f in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = TRAIN[0]\n",
    "B = TEST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((912968, 3), (225592, 3))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.226817e+03 8.211416e+03 4.823422e-01] [1.9323999e+03 1.9287136e+03 4.8382035e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 6.2944175e+03,  6.2827021e+03, -1.4781356e-03], dtype=float32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.mean(A, axis=0), np.mean(B, axis=0))\n",
    "np.mean(A, axis=0) - np.mean(B, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0784928e+07 2.0876456e+07 2.4968819e-01] [1.3218791e+06 1.3093082e+06 2.4973819e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.9463048e+07,  1.9567148e+07, -4.9993396e-05], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.var(A, axis=0), np.var(B, axis=0))\n",
    "np.var(A, axis=0) - np.var(B, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.0784928e+07, 2.0876456e+07, 2.4968819e-01], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sci.stats.moment(A, 2, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph representations\n",
    "\n",
    "Need a way to embed tensorflow graphs?!\n",
    "Rather than using hand designed features such as; the number of params, depth, width, nonlinearity, ... it would  make more sense to learn which features are important for learnability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_graph():\n",
    "    pass\n",
    "\n",
    "def embed_graph():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta learning\n",
    "\n",
    "Would be cool. To use all the data generated. Model hyperparams, data complexity and learnability to train a metalearner that predicts/optimises learnability. We could then try to interpret what it has learned!?\n",
    "\n",
    "Define learnability as ???\n",
    "\n",
    "\n",
    "E.g. \n",
    "3 x lr, 3 x init, 3 x batch = 27 runs. Stop when learning has slowed to ??.\n",
    "\n",
    "- Var(runs) = how reliably can it be learned? (a function of )\n",
    "- Mean(runs) = how well can it be learned? (a function of the representation? aaaand the optimisation, and the data sampled...)\n",
    "- Mean(n_iters of each run?) = how much does it cost to learn?\n",
    "- Var(h_params) = how easy it is to find a good setting of hparams...\n",
    "- ?\n",
    "\n",
    "Learnability = easy + quality + cost?\n",
    "Or just predict each seprarately?\n",
    "\n",
    "Ideally want a dataset of 10,000. That would require, say, 100 datasets, and 100 function approximators. Giving a total of 10,000 * 27 runs. If each takes approx 5 min (optimistically), then that totals nearly 100 days of compute time..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Leveraging existing data?\n",
    "\n",
    "Algorithms that work on given problems are the therorists data. \n",
    "Dataset; \n",
    "`(approximator, optimiser, data, loss)`\n",
    "* (AlexNet, SGD, ImageNet, Accuracy - 0.95)\n",
    "* (LeNet, ImageNet, Accuracy - 0.96)\n",
    "* (...)\n",
    "* (Seq2seq w atten, translation dataset, BLEU - ?)\n",
    "* \n",
    "\n",
    "Aka, meta learning...? So, lets go create our own dataset..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
