{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Exploring the motivations for (S)LT, the theorems that have come out of it and the topics of interest. -->\n",
    "\n",
    "## Statistical learning theory\n",
    "\n",
    "\n",
    "\n",
    "So what is the goal of statistical learning theory research?\n",
    "\n",
    "> \n",
    "- establish conditions for generalization\n",
    "_ unifying various settings (showing they are fundamentally trying to solve the same problem)?\n",
    "- does optimising our loss get us where we want to go? Is the global minima reachable?\n",
    "- what is necessary/sufficient for us to find the minima? (well we need to see some data, we need some compute, ...)\n",
    "- what causes generalisation, and how can we control generalisation?\n",
    "\n",
    "using statistical methods. Ok then, what are statistical methods? OR is that not what they mean by statistical?\n",
    "\n",
    "\n",
    "## Derivation\n",
    "\n",
    "How can you measure progress in learning? Candidates are;\n",
    "* get your current performance via some oracle (aka ERM)\n",
    "* the sum of your past and current performance (aka regret?)\n",
    "* the difference between your past and current performance (aka ?)\n",
    "* ??\n",
    "\n",
    "\n",
    "Traditional setting\n",
    "Assumptions.\n",
    "Exists a function t(.) that can return labels. Do we need to assume this is deterministic?\n",
    "\n",
    "\n",
    "Problem is,\n",
    "$$\n",
    "\\mathop{min}_\\theta \\mathbb E_{x, y \\sim \\mathcal D} [\\mathcal L(x, y, \\theta)]\n",
    "$$\n",
    "\n",
    "So in the case where we have a dataset, we can refoumulate this as \n",
    "\n",
    "$$\n",
    "\\mathop{min}_\\theta \\frac{1}{N}  \\sum_{i=0}^N \\mathcal L(x_i, y_i, \\theta)]\n",
    "$$\n",
    "\n",
    "iff. The data we have is independently and identically sampled.\n",
    "This comes back to the expectation, we care about doing well in expectation. So we might not care much if we do poorly on something very rare.\n",
    "\n",
    "\n",
    "### But what can it do for me?\n",
    "\n",
    "Imagine you want to be able to translate between two langauges, you have a seq2seq model and a lot of parallel data. You train, and train, you optimise hyperparameters, ...\n",
    "But it doesn't work well.\n",
    "\n",
    "Theory to the rescue! ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probably approximately correct\n",
    "\n",
    "what is it missing?\n",
    "\n",
    "`Probably`: given that you have sampled the data, does it contain enough info about the function you want to learn\n",
    "\n",
    "`Approximately`: does your learned fuction achieve epsilon-error?\n",
    "\n",
    "\n",
    "Efficiently, probably, approximately, correct.\n",
    "\n",
    "In practice, we dont care so much about the `probably` part? We can just restart from another init, and hope we sample the data 'better'. This is a result of many real world applications using offline training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Satisfiability\n",
    "\n",
    "Learning as a satisfiability problem. Satisfying your data. But using an inductive approach rather than ?.  \n",
    "\n",
    "Is there are smooth transition between ~correct and satisfiable algols? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glossary\n",
    "### Overfitting\n",
    "\n",
    "__NOISE__\n",
    "Overfitting isnt a problem if there is no noise... Then we are just fitting the function.\n",
    "Does adding noise always make the function more complex? No(example!)? This means Occams razor could sometimes be wrong!?\n",
    "Is it possible that there is a setting we increased complexity is a good thing? Actually we do this quite often. We can fit somewhat inaccurate models to data, but we assume there is a more complex 'true' model that is still unknown.\n",
    "\n",
    "Alternative definitions of overfitting.\n",
    "In RL we can infinitely sample from the environment, making it impossible to overfit (why? eventually will marginalise out the noise, but with momentum this might not be true? or stuck in a local minima?). But, in another sense we can still overfit?\n",
    "\n",
    "Alternative.\n",
    "Without noise, we are doing something closer to solving a matrix by elimiation. But the problem is using underconstrains. But, if it was over constrained, we could find a solution that only satisfies a subset of the data. This is reminicent of overitting as well. \n",
    "\n",
    "## Generalisation\n",
    "\n",
    "Disambiguations? \n",
    "* Training-test set. \n",
    "* apply to a different domain.\n",
    "* unifying many domains.\n",
    "* \n",
    "\n",
    "Learning something more abstract than what solves the problem you are given (sounds like meta learning?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Machine learning theory as meta-learning\n",
    "\n",
    "Algorithms that work on given problems are the therorists data. \n",
    "Dataset; \n",
    "`(approximator, optimiser, data, loss)`\n",
    "* (AlexNet, SGD, ImageNet, Accuracy - 0.95)\n",
    "* (LeNet, ImageNet, Accuracy - 0.96)\n",
    "* (...)\n",
    "* (Seq2seq w atten, translation dataset, BLEU - ?)\n",
    "* \n",
    "\n",
    "Aka, meta learning...?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
