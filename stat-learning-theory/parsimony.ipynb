{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occam's razor\n",
    "\n",
    "Initially I was confused as there seem to be two distinct senses of _\"complexity\"_ used in regard to occams razor.\n",
    "\n",
    "* The first is a complex phonomena or hypothesis (e.g. that we can use thousands/millions/lots of transistors to control a space ship landing on Mars). Which is a measure of how many interacting parts there are within a system. Could also be measured as the magnitide of the jacobian of outputs w.r.t inputs.\n",
    "* and the second is a model's complexity or the complexity of a hypothesis class (e.g. a neural network with a billion parameters). Which is really a measure of flexibility and explanatory power. \n",
    "\n",
    "To contrast the two definitions, it is about the amount control we have over the complex system. We have little control over transistors etc as they are all fully specified, their roles accounted for, their definition nailed down. However, each weight in a neural network is free to vary, giving us lots of control and thus flexibility. That is the complexity we are talking about here.\n",
    "\n",
    "Also, it should be noted that occam's razor seems to have a couple of definitions. Here I consider occam's razor as meaning that we should prefer simpler hypotheses. Alternatively, it is also understood to say that we should prefer explanations with fewer assumptions. Which is not necessarily the same thing.\n",
    "\n",
    "### Automation\n",
    "\n",
    "> How does the posterior of a model automatically embody occams razor? \n",
    "\n",
    "Bayes theorem does it for free. Because complex (aka flexible) hypotheses are able to explain more data, their probability distribution over the data is more spread out. This is necessary as the sum rule tells us that $\\sum_i P(x_i\\mid h) = 1$. For example consider a coin flip. A simple hypothesis could say that all \n",
    "\n",
    "Another nice feature of occams razor is that simple (inflexible) hypotheses are more easily falsified. The more powerful a model/explanation is the more wiggle room we have to explain something, we can alter the parameters/details. A simple precise model doesnt allow much, if any, wiggle room. Thus a simple negative test can be damning for the simple model as it has no where to run, no flexibility to allow it to morph. \n",
    "\n",
    "\n",
    "### Regularisation\n",
    "\n",
    "I think Mackay has just moved the goal posts. We still need to have priors on each of the parameters within a hypothesis/model, rather than on each of our hypothesis. But each hypothesis is a combination of its parameters so in some senses the priors on parameters are priors on the allowable hypotheses.\n",
    "\n",
    "I.e. We dont need to estimate $P(\\mathcal H)$, but $\\forall \\theta \\in \\mathcal H_i$ we need $P(\\theta \\mid \\mathcal H_i)$. (It makes me think of a hierarchy of priors)\n",
    "\n",
    "\n",
    "### Thoughts\n",
    "\n",
    "Explanations/proofs I would like to see.\n",
    "\n",
    "* In the most general setting bayesian occam's razor is equivalent to kolmogorov complexity and therefore is uncomputable.\n",
    "* That models with more parameters are necessarily more flexible.\n",
    "* Deep nets are better than shallow ones because deep nets can achieve more complex hypotheses while remaining relatively inflexibile. A wide net would need many more parameters to achieve the same level of complexity, but this would necessitate that more flexibility is also added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah. So we want to match complexity with as little flexibility as possible. (This hints as deep?)\n",
    "\n",
    "\n",
    "A concrete example.\n",
    "\n",
    "parameterised relus versus vanilla relu. same representational capacity/complexity, different learnability/flexibility. not true?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
